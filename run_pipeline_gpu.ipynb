{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 집값 예측 - GPU 서버 파이프라인\n",
                "\n",
                "이 노트북은 GPU 서버 환경에서 실행되도록 설계되었습니다.\n",
                "API를 이용한 좌표 보완과 **KNN 결측치 보완**을 포함한 전체 전처리 과정을 수행합니다.\n",
                "\n",
                "## 설정 및 경로 구성\n",
                "다음과 같은 디렉토리 구조를 확인하거나, 아래 코드에서 `DATA_DIR`을 알맞게 수정하세요:\n",
                "\n",
                "```\n",
                "project_root/\n",
                "├── 병합/ (현재 디렉토리)\n",
                "│   └── run_pipeline_gpu.ipynb\n",
                "├── data/\n",
                "│   └── raw/\n",
                "│       ├── train.csv\n",
                "│       └── test.csv\n",
                "```\n",
                "\n",
                "> **참고:** 서버에 `rapids` (cuML) 라이브러리가 설치되어 있다면, `sklearn.impute.KNNImputer` 대신 `cuml.preprocessing.KNNImputer`를 사용하여 GPU 가속을 활용할 수 있습니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "import requests\n",
                "import time\n",
                "from tqdm import tqdm\n",
                "from sklearn.impute import KNNImputer\n",
                "\n",
                "# GPU 사용 가능 여부 확인 (선택 사항 - 정보 확인용)\n",
                "try:\n",
                "    import torch\n",
                "    if torch.cuda.is_available():\n",
                "        print(f\"[정보] GPU 감지됨: {torch.cuda.get_device_name(0)}\")\n",
                "    else:\n",
                "        print(\"[정보] GPU가 감지되지 않았습니다. CPU로 실행합니다.\")\n",
                "except ImportError:\n",
                "    print(\"[정보] PyTorch가 설치되지 않았습니다. (GPU 확인 생략)\")\n",
                "\n",
                "# ==========================================\n",
                "# 1. 데이터 경로 자동 탐지\n",
                "# ==========================================\n",
                "# 다양한 환경(로컬, 서버 등)에 대응하기 위해 여러 예상 경로를 확인합니다.\n",
                "POSSIBLE_PATHS = [\n",
                "    '../data/raw',       # 표준 프로젝트 구조\n",
                "    'data/raw',          # 실행 폴더 내에 data가 있는 경우\n",
                "    '../../data/raw',    # 더 깊은 경로에 있는 경우\n",
                "    './'                 # 현재 디렉토리에 파일이 있는 경우\n",
                "]\n",
                "\n",
                "DATA_DIR = None\n",
                "for path in POSSIBLE_PATHS:\n",
                "    if os.path.exists(os.path.join(path, 'train.csv')):\n",
                "        DATA_DIR = path\n",
                "        break\n",
                "\n",
                "if DATA_DIR is None:\n",
                "    raise FileNotFoundError(\"표준 경로에서 'train.csv'를 찾을 수 없습니다. DATA_DIR을 수동으로 설정해주세요.\")\n",
                "\n",
                "print(f\"데이터 디렉토리 확인됨: {DATA_DIR}\")\n",
                "TRAIN_PATH = os.path.join(DATA_DIR, 'train.csv')\n",
                "TEST_PATH = os.path.join(DATA_DIR, 'test.csv')\n",
                "\n",
                "# ==========================================\n",
                "# 2. API 설정\n",
                "# ==========================================\n",
                "\n",
                "# 보안을 위해 환경 변수에서 먼저 API 키를 로드합니다.\n",
                "KAKAO_API_KEY = os.environ.get(\"KAKAO_API_KEY\")\n",
                "\n",
                "# 환경 변수에 키가 없을 경우, 하드코딩된 키를 사용합니다. (테스트 편의성)\n",
                "if not KAKAO_API_KEY:\n",
                "    KAKAO_API_KEY = \"50721163f60b5e5c192f6c3847602b05\"  \n",
                "    print(\"경고: 하드코딩된 API 키를 사용합니다. (유효한 키인지 확인 필요)\")\n",
                "else:\n",
                "    print(\"환경 변수에서 API 키를 로드했습니다.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 단계 1: 취소된 거래 제거"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def step1_filter_cancelled(df):\n",
                "    \"\"\"\n",
                "    취소된 거래(해제사유발생일이 존재하는 데이터)를 제거합니다.\n",
                "    \"\"\"\n",
                "    print(\"[단계 1] 취소된 거래 필터링 중...\")\n",
                "    if '해제사유발생일' in df.columns:\n",
                "        n_cancelled = df['해제사유발생일'].notnull().sum()\n",
                "        if n_cancelled > 0:\n",
                "            print(f\" -> {n_cancelled}건의 취소된 거래를 발견했습니다. 제거합니다...\")\n",
                "            # 해제사유발생일이 비어있는(정상 거래) 데이터만 유지\n",
                "            df = df[df['해제사유발생일'].isnull()].copy()\n",
                "            # 컬럼 삭제\n",
                "            df.drop(columns=['해제사유발생일'], inplace=True)\n",
                "    return df\n",
                "\n",
                "# 데이터 로드\n",
                "print(\"데이터 로딩 중...\")\n",
                "df_train = pd.read_csv(TRAIN_PATH)\n",
                "print(f\"학습 데이터 크기: {df_train.shape}\")\n",
                "\n",
                "df_test = pd.read_csv(TEST_PATH)\n",
                "print(f\"테스트 데이터 크기: {df_test.shape}\")\n",
                "\n",
                "df_train = step1_filter_cancelled(df_train)\n",
                "df_test = step1_filter_cancelled(df_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 단계 2~7: 전처리 파이프라인 (타입 변환, 클리닝, 특성 공학)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocessing_pipeline(df):\n",
                "    \"\"\"\n",
                "    기본적인 전처리 과정을 순차적으로 수행합니다.\n",
                "    1. 자료형 변환 (Float -> Int64)\n",
                "    2. 날짜 파싱 (계약년월 + 계약일 -> 계약일자)\n",
                "    3. 주소 파싱 및 지번주소 생성\n",
                "    4. 불필요한 컬럼 제거\n",
                "    \"\"\"\n",
                "    \n",
                "    # 1. 자료형 변환 (메모리 최적화를 위해 Float을 Int64로 변환)\n",
                "    int_cols = [\n",
                "        '본번', '부번', 'k-전체동수', 'k-전체세대수', '주차대수',\n",
                "        'k-연면적', 'k-주거전용면적', 'k-관리비부과면적',\n",
                "        'k-전용면적별세대현황(60㎡이하)', 'k-전용면적별세대현황(60㎡~85㎡이하)', \n",
                "        'k-85㎡~135㎡이하', 'k-135㎡초과'\n",
                "    ]\n",
                "    for col in int_cols:\n",
                "        if col in df.columns:\n",
                "            try:\n",
                "                # 반올림 후 Int64(Nullable Int)로 변환\n",
                "                df[col] = df[col].round().astype('Int64')\n",
                "            except: pass\n",
                "\n",
                "    # 2. 날짜 파싱\n",
                "    if '계약년월' in df.columns and '계약일' in df.columns:\n",
                "        # 계약년월(202301) + 계약일(1 -> 01) 합쳐서 하나의 정수형 날짜(20230101)로 생성\n",
                "        df['계약일자'] = (df['계약년월'].astype(str) + df['계약일'].astype(str).str.zfill(2)).astype('Int64')\n",
                "        df.drop(columns=['계약일'], inplace=True, errors='ignore')\n",
                "\n",
                "    # 3. 주소 파싱 (시군구 분리 및 지번주소 재조합)\n",
                "    if '시군구' in df.columns:\n",
                "        split = df['시군구'].str.split(' ', expand=True)\n",
                "        if split.shape[1] >= 2: df['구'] = split[1]\n",
                "        if split.shape[1] >= 3: df['동'] = split[2]\n",
                "    \n",
                "    if '본번' in df.columns and '부번' in df.columns:\n",
                "        try:\n",
                "            # 결측치를 0으로 채우고 문자열로 변환\n",
                "            bon = df['본번'].fillna(0).astype(int).astype(str)\n",
                "            bu = df['부번'].fillna(0).astype(int).astype(str)\n",
                "            \n",
                "            # 벡터화 연산으로 주소 조합 (시군구 + 본번 + [-부번])\n",
                "            full_addr = df['시군구'] + \" \" + bon\n",
                "            mask_bu = (df['부번'].fillna(0) > 0)\n",
                "            full_addr.loc[mask_bu] += \"-\" + bu.loc[mask_bu]\n",
                "            \n",
                "            df['지번주소'] = full_addr\n",
                "            # 사용된 원본 컬럼 제거\n",
                "            df.drop(columns=['본번', '부번'], inplace=True)\n",
                "        except Exception as e:\n",
                "            print(f\"주소 생성 중 경고 발생: {e}\")\n",
                "\n",
                "    # 4. 불필요한 컬럼 제거\n",
                "    # 분석 결과 유의미하지 않거나 정보가 부족한 컬럼들\n",
                "    drop_cols = [\n",
                "        'k-등록일자', 'k-홈페이지', 'k-수정일자', 'k-전화번호', 'k-팩스번호',\n",
                "        '고용보험관리번호', '단지신청일', '단지승인일', '사용허가여부', '관리비 업로드',\n",
                "        '기타/의무/임대/임의=1/2/3/4', '단지소개기존clob', '건축면적',\n",
                "        '계약년월', 'k-주거전용면적', '번지', '시군구',\n",
                "        '중개사소재지', '등기신청일자', '도로명', 'k-시행사', \n",
                "        'k-단지분류(아파트,주상복합등등)', 'k-관리방식',\n",
                "        'k-전용면적별세대현황(60㎡이하)', 'k-전용면적별세대현황(60㎡~85㎡이하)',\n",
                "        'k-85㎡~135㎡이하', 'k-135㎡초과'\n",
                "    ]\n",
                "    df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)\n",
                "\n",
                "    # 컬럼명 정리\n",
                "    rename_map = {'k-사용검사일-사용승인일': '사용검사일'}\n",
                "    df.rename(columns=rename_map, inplace=True)\n",
                "\n",
                "    return df\n",
                "\n",
                "print(\"전처리 파이프라인 적용 중...\")\n",
                "df_train = preprocessing_pipeline(df_train)\n",
                "df_test = preprocessing_pipeline(df_test)\n",
                "print(\"완료.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 단계 8 & 9: 좌표 보완 (API 이용)\n",
                "**참고:** API 속도가 느리거나 제한에 걸린다면, 이 단계를 중단하고 다음 단계로 넘어가세요. (중단 시점까지의 데이터는 메모리에 남아있습니다)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_coords(addr):\n",
                "    \"\"\"카카오 API를 사용하여 주소의 좌표를 가져옵니다.\"\"\"\n",
                "    url = \"https://dapi.kakao.com/v2/local/search/address.json\"\n",
                "    headers = {\"Authorization\": f\"KakaoAK {KAKAO_API_KEY}\"}\n",
                "    try:\n",
                "        res = requests.get(url, headers=headers, params={\"query\": addr}, timeout=3)\n",
                "        if res.status_code == 200:\n",
                "            docs = res.json().get('documents')\n",
                "            if docs: return float(docs[0]['x']), float(docs[0]['y'])\n",
                "    except:\n",
                "        pass\n",
                "    return None, None\n",
                "\n",
                "def fill_coordinates(df):\n",
                "    \"\"\"결측된 좌표 정보를 API를 통해 보완합니다.\"\"\"\n",
                "    print(\"좌표 보완 작업 시작...\")\n",
                "    \n",
                "    # 결측치 확인\n",
                "    missing = df[(df['좌표X'].isnull()) | (df['좌표X'] == 0)]\n",
                "    if len(missing) == 0:\n",
                "        print(\" -> 결측된 좌표가 없습니다.\")\n",
                "        return df\n",
                "        \n",
                "    print(f\" -> 결측 건수: {len(missing)} 건\")\n",
                "    unique_addr = missing['지번주소'].unique()\n",
                "    cache = {}\n",
                "    \n",
                "    # 일괄 처리 (API 호출)\n",
                "    for addr in tqdm(unique_addr, desc=\"API 호출 중\"):\n",
                "        if pd.isna(addr): continue\n",
                "        cache[addr] = get_coords(str(addr))\n",
                "        \n",
                "    # 결과 적용\n",
                "    for idx in missing.index:\n",
                "        addr = df.at[idx, '지번주소']\n",
                "        x, y = cache.get(addr, (None, None))\n",
                "        if x:\n",
                "            df.at[idx, '좌표X'] = x\n",
                "            df.at[idx, '좌표Y'] = y\n",
                "            \n",
                "    return df\n",
                "\n",
                "# 실행\n",
                "print(\"학습 데이터 좌표 보완 중...\")\n",
                "df_train = fill_coordinates(df_train)\n",
                "print(\"테스트 데이터 좌표 보완 중...\")\n",
                "df_test = fill_coordinates(df_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 단계 10: KNN 결측치 보완 (좌표 기반)\n",
                "\n",
                "**GPU 가속 팁:**\n",
                "만약 `cuml` 라이브러리(RAPIDS)가 설치되어 있다면, 아래 코드를 사용하여 더 빠르게 실행할 수 있습니다:\n",
                "```python\n",
                "from cuml.preprocessing import KNNImputer\n",
                "```\n",
                "그렇지 않은 경우 일반 `sklearn` (CPU) 방식이 사용됩니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_knn_imputation(df, k=5):\n",
                "    \"\"\"\n",
                "    좌표(X, Y) 정보를 활용하여 이웃한 아파트의 정보로 수치형 결측치를 보완합니다.\n",
                "    \"\"\"\n",
                "    print(\"[KNN] 결측치 보완 작업 시작...\")\n",
                "    \n",
                "    # 보완할 대상 컬럼 (타겟)\n",
                "    target_cols = ['주차대수', '건축년도', 'k-전체세대수', 'k-연면적']\n",
                "    # 거리 계산에 사용할 컬럼 (피쳐)\n",
                "    feature_cols = ['좌표X', '좌표Y']\n",
                "    \n",
                "    # KNN 작동을 위해 좌표가 반드시 있어야 합니다. \n",
                "    # 여전히 결측인 좌표는 평균값으로 임시 대치합니다.\n",
                "    for c in feature_cols:\n",
                "        if df[c].isnull().sum() > 0:\n",
                "            print(f\" -> {c} 컬럼에 남은 결측치가 있어 평균값으로 대치합니다. (KNN 안전장치)\")\n",
                "            df[c].fillna(df[c].mean(), inplace=True)\n",
                "            \n",
                "    # 실제 데이터프레임에 존재하는 타겟 컬럼만 선택\n",
                "    valid_targets = [c for c in target_cols if c in df.columns]\n",
                "    if not valid_targets: return df\n",
                "    \n",
                "    # 데이터 준비\n",
                "    use_cols = feature_cols + valid_targets\n",
                "    impute_data = df[use_cols].copy()\n",
                "    \n",
                "    # KNN 실행\n",
                "    print(f\" -> 로직: {len(impute_data)}개 데이터에 대해 KNN(k={k}) 수행. 타겟: {valid_targets}\")\n",
                "    imputer = KNNImputer(n_neighbors=k)\n",
                "    out_data = imputer.fit_transform(impute_data)\n",
                "    \n",
                "    # 결과 적용\n",
                "    out_df = pd.DataFrame(out_data, columns=use_cols, index=impute_data.index)\n",
                "    for col in valid_targets:\n",
                "        df[col] = out_df[col]\n",
                "        \n",
                "    print(\" -> 완료.\")\n",
                "    return df\n",
                "\n",
                "print(\"--- 학습 데이터 보완 ---\")\n",
                "df_train = run_knn_imputation(df_train)\n",
                "print(\"--- 테스트 데이터 보완 ---\")\n",
                "df_test = run_knn_imputation(df_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 최종 결과 저장"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 저장 경로 설정 (data/processed 폴더)\n",
                "SAVE_DIR = os.path.join(DATA_DIR, 'processed')\n",
                "if not os.path.exists(SAVE_DIR):\n",
                "    os.makedirs(SAVE_DIR)\n",
                "    \n",
                "train_save = os.path.join(SAVE_DIR, 'train_final.csv')\n",
                "test_save = os.path.join(SAVE_DIR, 'test_final.csv')\n",
                "\n",
                "df_train.to_csv(train_save, index=False)\n",
                "df_test.to_csv(test_save, index=False)\n",
                "\n",
                "print(\"모든 전처리 과정이 완료되었습니다!\")\n",
                "print(f\"저장된 경로:\\n  {train_save}\\n  {test_save}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}